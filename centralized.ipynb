{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474d81b3",
   "metadata": {},
   "source": [
    "Performing sentiment analysis\n",
    "Dataset used: https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment\n",
    "sentiments = {\n",
    "    \"LABEL_0\": \"Bearish\", \n",
    "    \"LABEL_1\": \"Bullish\", \n",
    "    \"LABEL_2\": \"Neutral\"\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72d27df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\susan\\anaconda3\\envs\\flwr\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, BertTokenizer, Trainer, TrainingArguments, DistilBertModel, DistilBertTokenizer, RobertaModel, RobertaTokenizer, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0648f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bcb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa95257",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fac6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(texts, tokenizer):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length = 128,\n",
    "        return_tensors='pt'  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c238b9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BYND - JPMorgan reels in expectations on Beyo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$CCL $RCL - Nomura points to bookings weakness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$CX - Cemex cut at Credit Suisse, J.P. Morgan ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ESS: BTIG Research cuts to Neutral https://t....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$FNKO - Funko slides after Piper Jaffray PT cu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9538</th>\n",
       "      <td>The Week's Gainers and Losers on the Stoxx Eur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>Tupperware Brands among consumer gainers; Unil...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9540</th>\n",
       "      <td>vTv Therapeutics leads healthcare gainers; Myo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>WORK, XPO, PYX and AMKR among after hour movers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>YNDX, I, QD and OESX among tech movers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9543 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     $BYND - JPMorgan reels in expectations on Beyo...      0\n",
       "1     $CCL $RCL - Nomura points to bookings weakness...      0\n",
       "2     $CX - Cemex cut at Credit Suisse, J.P. Morgan ...      0\n",
       "3     $ESS: BTIG Research cuts to Neutral https://t....      0\n",
       "4     $FNKO - Funko slides after Piper Jaffray PT cu...      0\n",
       "...                                                 ...    ...\n",
       "9538  The Week's Gainers and Losers on the Stoxx Eur...      2\n",
       "9539  Tupperware Brands among consumer gainers; Unil...      2\n",
       "9540  vTv Therapeutics leads healthcare gainers; Myo...      2\n",
       "9541    WORK, XPO, PYX and AMKR among after hour movers      2\n",
       "9542             YNDX, I, QD and OESX among tech movers      2\n",
       "\n",
       "[9543 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "train_data = train_data.dropna(subset=['text', 'label'])\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1dc92cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_feature.shape: (9543,)\n"
     ]
    }
   ],
   "source": [
    "train_data_feature = train_data['text'].values\n",
    "train_data_label = train_data['label'].values\n",
    "print(\"train_data_feature.shape:\", train_data_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370027d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_feature.shape: (2388,)\n"
     ]
    }
   ],
   "source": [
    "test_data_feature = test_data['text'].values\n",
    "test_data_label = test_data['label'].values\n",
    "print(\"test_data_feature.shape:\", test_data_feature.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf74c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer_function(train_data_feature.tolist(), distilbert_tokenizer)\n",
    "val_encoding = tokenizer_function(test_data_feature.tolist()[:1000], distilbert_tokenizer)\n",
    "test_encodings = tokenizer_function(test_data_feature.tolist(), distilbert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5ac9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(train_encodings, train_data_label.tolist())\n",
    "val_dataset = SentimentDataset(val_encoding, test_data_label.tolist()[:1000])\n",
    "test_dataset = SentimentDataset(test_encodings, test_data_label.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8710f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_classes):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.bert = pretrained_model\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()  # Multi-class loss\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels.long())  # labels should be class indices (int)\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "pre_trained_distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model = CustomBertModel(pre_trained_distilbert_model, num_classes = 3).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10e7175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)  # <--- Correct for multiclass\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b7af577",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bertResults\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    "    # warmup_ratio=0.06,  # Prevent aggressive weight updates early\n",
    "    # gradient_accumulation_steps=2,  # Simulate larger batch without increasing memory\n",
    "    # fp16=True,  # Use mixed precision training if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "983b904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6578ee94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5970' max='5970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5970/5970 20:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>0.433496</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.839609</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.838412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.321800</td>\n",
       "      <td>0.387157</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.866250</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.866100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.246300</td>\n",
       "      <td>0.398612</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.868427</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.868681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.473238</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.870823</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.871228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.536488</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.871022</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.871333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.616849</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.874773</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.874287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.664598</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.876049</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.876001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.721075</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.870274</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.869223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.729405</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.878603</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.878713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.730904</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.881067</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.881005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 73022.922 minutes\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"/models/centralized_bert_model\")\n",
    "\n",
    "# output the training time\n",
    "training_time = trainer.state.log_history[-1]['train_runtime']\n",
    "print(f\"Training time: {training_time * 60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e671135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.7099828124046326, 'eval_accuracy': 0.8806532663316583, 'eval_precision': 0.8821743626009017, 'eval_recall': 0.8806532663316583, 'eval_f1': 0.8811056535334082, 'eval_runtime': 8.7868, 'eval_samples_per_second': 271.772, 'eval_steps_per_second': 17.071, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "distilbert_test_results = trainer.evaluate(test_dataset)\n",
    "print(f'Test Results: {distilbert_test_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc9c121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 20.284145000000002 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training time: {training_time / 60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39474f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
